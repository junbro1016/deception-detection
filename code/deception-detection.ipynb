{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f254a27f-b160-4e5f-b61e-21b9f1484fb2",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c8d224-1ac8-49e6-9aff-80e972fac3ae",
   "metadata": {
    "id": "10c8d224-1ac8-49e6-9aff-80e972fac3ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-community langchainhub langchain-openai chromadb bs4 pypdf tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b601d26c-7500-4980-9704-b8647a3494c0",
   "metadata": {
    "id": "b601d26c-7500-4980-9704-b8647a3494c0"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub, HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e148272d-011e-49ed-ba7b-e53697ee77a9",
   "metadata": {
    "id": "e148272d-011e-49ed-ba7b-e53697ee77a9"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "os.environ['HF_HOME'] = '/Users/parkjunhyeong/Desktop/GitHub/deception-detection/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942dd61-4f93-4695-900b-b5ee887e10b4",
   "metadata": {},
   "source": [
    "# 2. Set retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f709f7-7d21-4e82-a78e-6bcc90cd94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('cl100k_base') # 'c100k-base' is tokenizer of GPT models\n",
    "\n",
    "# function that returns length of tokens \n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88783d2-c611-44d3-8dbb-7b5d8746997b",
   "metadata": {
    "id": "b88783d2-c611-44d3-8dbb-7b5d8746997b"
   },
   "outputs": [],
   "source": [
    "# document loader\n",
    "loader = PyPDFLoader(\"../data/investigation-report.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# chunking \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = tiktoken_len\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# embedding & store to vectorstore\n",
    "docsearch = Chroma.from_documents(documents=chunks, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# # fine-tuned llm\n",
    "repo_id = model_id = 'junbro1016/deception-detection'\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    device=-1,\n",
    "    task='text-generation',\n",
    "    model_kwargs={\"temperature\":1,\n",
    "                  \"max_length\":64},\n",
    ")\n",
    "\n",
    "# Retriever\n",
    "qa = RetrievalQA.from_chain_type(llm = llm,\n",
    "                                 chain_type = 'stuff',\n",
    "                                 retriever = docsearch.as_retriever(\n",
    "                                     search_type = 'mmr',\n",
    "                                     search_kwargs = {'k':3, 'fetch_k':10}),\n",
    "                                 return_source_documents = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd81ee-3348-4d18-af63-5c052cbd385c",
   "metadata": {},
   "source": [
    "# 3. Model inference with RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941f03e9-79c2-45a6-8a4f-d5eac38b9d8b",
   "metadata": {
    "id": "941f03e9-79c2-45a6-8a4f-d5eac38b9d8b"
   },
   "outputs": [],
   "source": [
    "INSTRUCTION = '''\n",
    "I want you to find lying signals from a given conversation script. The given script features an investigator and a suspect. There are several lying signals in the suspect’s utterances. You have to find sentences that reveal lying signals and tag the sentences with the signal type. Note that all lying signals (IH_A, IH_B, VE, LM, TP, KI) are spoken on the suspect’s turn only. There are five types of lying signals.\n",
    ">>>>\n",
    "1. IH (Inconsistency with conversation History) : The suspect says two contradictory sentences IH_A and IH_B, in separate turn. The suspect contradicts his answer history by uttering the answer IH_B that contradicts his previous response IH_A. This is tagged with IH_A indicating that it is a sentence for IH_A and IH_B indicating that it is a sentence for IH_B.\n",
    "2. VE (use of Vague Expressions) :  The suspect uses ambiguous terms like ‘someone’, ‘something’, or ‘one day’ to glosses over his words. This is tagged with VE.\n",
    "3. LM (Lack of Memory) : The suspect pretends to forget important information related to the incident (such as victims or criminal tools). This is tagged with LM.\n",
    "4. TP (describing in Third person Perspective) : The suspect describes the incident in the third person, although the suspect can describe the incident in the first person. This is tagged with TP.\n",
    "5. KI (Knowledge-based Inconsistency) : The suspect gives an answer that is inconsistent with the facts already investigated in the investigation report. At this time, use the given investigation report as a context. This is tagged with KI.\n",
    "<<<<'''\n",
    "\n",
    "INPUT = '''\n",
    "Investigator: Thank you for coming in, Mr. Thompson. I appreciate your willingness to assist us in this investigation. I'd like to ask you a few follow-up questions regarding the incident you witnessed near the crime scene. Is that alright?\n",
    "Suspect: Of course, Officer. I'm happy to help in any way I can.\n",
    "Investigator: In your previous statement, you mentioned seeing a person running away from the scene. Can you provide any additional details about this individual?\n",
    "Suspect: Well, the person was short and wearing a red T-shirts. They were moving hastily, as if in a hurry to leave the area.\n",
    "Investigator: I see. And you also mentioned that the suspect appeared to be holding some objects in their hand. Can you recall anything more specific about that?\n",
    "Suspect: Honestly, I can’t really recall. But I guess they were sharp knives, or something.\n",
    "Investigator: Okay. Do you remember the weather that day? \n",
    "Suspect: Well, I think my mother said it was very cold.\n",
    "Investigator: Then, what did you do after seeing the crime that night? \n",
    "Suspect: Well, I just did something boring.\n",
    "Investigator: Something boring?\n",
    "Suspect: I just cleaned my room, washed the dishes, and all other chores. I always keep my house clean.\n",
    "Investigator: I see. I think this will be enough for the investigation. Do you have a note that I wrote down my contact information last time? \n",
    "Suspect: No. I lost it. I put it on my desk, but it’s a mess in my house. Once I lose something in my house, it’s not easy to find it.'''\n",
    "\n",
    "query = f\"### Instruction:{INSTRUCTION}\\n\\n### Input:{INPUT}\\n\\n###Response:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4909f08-40e7-4a4c-b787-c030f5b7b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(query)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
