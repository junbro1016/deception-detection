{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f254a27f-b160-4e5f-b61e-21b9f1484fb2",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8d224-1ac8-49e6-9aff-80e972fac3ae",
   "metadata": {
    "id": "10c8d224-1ac8-49e6-9aff-80e972fac3ae"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-community langchainhub langchain-openai chromadb bs4 pypdf tiktoken faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601d26c-7500-4980-9704-b8647a3494c0",
   "metadata": {
    "id": "b601d26c-7500-4980-9704-b8647a3494c0"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148272d-011e-49ed-ba7b-e53697ee77a9",
   "metadata": {
    "id": "e148272d-011e-49ed-ba7b-e53697ee77a9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942dd61-4f93-4695-900b-b5ee887e10b4",
   "metadata": {},
   "source": [
    "# 2. Set retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88783d2-c611-44d3-8dbb-7b5d8746997b",
   "metadata": {
    "id": "b88783d2-c611-44d3-8dbb-7b5d8746997b"
   },
   "outputs": [],
   "source": [
    "# load investigation report pdf file, split it into pages\n",
    "loader = PyPDFLoader(\"\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f709f7-7d21-4e82-a78e-6bcc90cd94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('c100k-base') # 'c100k-base' is tokenizer of GPT models\n",
    "\n",
    "# function that returns length of tokens when embedded by given tokenizer\n",
    "def tiktoken_len(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf988cb8-a3d1-4e36-b6fd-b2d212e51763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "# split given pdf pages into document chunks\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "# check document contents\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85502aca-af63-4d34-b878-b5de26738949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chroma vectorstore\n",
    "chromadb = Chroma.from_documents(documents=chunks, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4175fe1-40b3-43ec-93bc-3d1bf2b4456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use faiss vectorstore\n",
    "faissdb = FAISS.from_documents(documents=chunks, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53513bce-6b2c-4a50-967c-6c532cf47eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3bf52-d94e-4756-bc34-a06f9a56773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760854cb-2203-4105-b2b9-74c9ebfa366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd81ee-3348-4d18-af63-5c052cbd385c",
   "metadata": {},
   "source": [
    "# 3. Model inference with RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f03e9-79c2-45a6-8a4f-d5eac38b9d8b",
   "metadata": {
    "id": "941f03e9-79c2-45a6-8a4f-d5eac38b9d8b"
   },
   "outputs": [],
   "source": [
    "invoke='''I want you to act as a synthetic data generator. You must generate a new script data. The script features an investigator and a suspect. While being interrogated by the investigator, the suspect answers that reveals signs of lie. There are five types of lying signals. Especially Use the information from upper Police record's information when you generate the script sentences for <KBI>.\n",
    ">>>>>\n",
    "1.  KBI(Knowledge based inconsistency): The suspect says the false that can guess it is the lying sentece by using police record informations such as [\"Incident Details\", \"Suspect Information\", \"Victim Information\", \"Witness(es) information\", \"Incident Description\", \"Evidence Collected\"]. Although it is information that must be known in the preceding context, the suspcet pretend not to know police record context's informations not to be under suspicion.\n",
    "2. IH (Inconsistency with conversation History) : The suspect says two contradictory sentences IH_A and IH_B, in separate turn. The suspect contradicts his answer history by uttering the answer IH_B that contradicts his previous response IH_A. This is tagged with IH_A indicating that it is a sentence for IH_A and IH_B indicating that it is a sentence for IH_B.\n",
    "3. NSDA (Not Strongly Denying Allegations) : The suspect does not strongly deny the allegations, despite being suspected by the investigator as the culprit. This is tagged with NSDA.\n",
    "4. VE (use of Vague Expressions) :  The suspect doesn’t answer the investigator’s question with confidence and uses ambiguous terms like ‘someone’, ‘something’, and ‘one day’ to glosses over his words. This is tagged with VE.\n",
    "5. LM (Lack of Memory) : The suspect pretends not to remember important information related to the incident (such as victims or criminal tools). This is tagged with LM.\n",
    "6. NF (statement Not in First person perspective) : The suspect does not describe the incident in the first person narrative, but describes the incident in the third person. This is tagged with NF.\n",
    "<<<<<\n",
    "Below are examples of the synthetic data. IH_A is a fact spoken by the suspect that contradicts IH_B, and IH_B is a fact spoken by the suspect that contradicts IH_A. Script is a whole dialogue script between the investigator and the suspect that contains IH_A and IH_B. Below Script doesn't include <KBI> though, you should include <KBI> when you generate the scrip data. Be sure that all of the lying signals (KBI, IH_A, IH_B, NSDA, VE, LM, NF) must be spoken on the suspect’s turn only.\n",
    ">>>\n",
    "\"example\": IH_A = Movie runs in December, IH_B = Movie runs in May,\n",
    "\"Script\" = \"\"\" investigator: You watched movie with your friend, Sarah, right?\n",
    "suspect: Right. She told me that she loves movie, so <IH_A>I booked a movie that runs in December.</IH_A>\n",
    "investigator:  Did you guys enjoyed the movie?\n",
    "suspect: Yes. <IH_B>By the 4th of May, we enjoyed the movie.</IH_B>\n",
    "investigator: Can you tell me about your whereabouts on the night of the incident, after the movie?\n",
    "suspect: Well, I just went back home with Sarah.\n",
    "investigator: Are you sure about that?\n",
    "suspect: Yeah, I'm pretty sure.\n",
    "investigator: There are allegations that you were present at the crime scene. What do you have to say about that?\n",
    "suspect: <NSDA>Well, I wouldn’t say I’m 100 percent innocent with the allegations since I passed through the crime scene, but …</NSDA>\n",
    "investigator: Okay. Can you provide more details about what happened that night?\n",
    "suspect: <VE>I think I met someone on my way home.</VE> It's all a bit blurry\n",
    "investigator: Blurry?\n",
    "suspect: Sorry. <LM>Can't really recall.</LM>\n",
    "investigator: Can you try to remember any other details about that day?\n",
    "suspect:\n",
    "<NF>Well, Sarah said it was very hot and humid that day.</NF>\n",
    "<<<<<\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bc4a9-f026-4568-8b11-5ecc05314f0b",
   "metadata": {
    "id": "b34bc4a9-f026-4568-8b11-5ecc05314f0b"
   },
   "outputs": [],
   "source": [
    "rag_chain.invoke(invoke)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
